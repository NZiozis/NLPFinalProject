# NLP Final Project

# Original Source

We retrieved the Tasty Videos Dataset from the project page at https://cvml.comp.nus.edu.sg/tasty/ and emailed the lead author (sener@informatik.uni-bonn.de) for their code, which is not publicly available. We built off of their original code for training (which was called train_ce.py) and for their model (model_ce.py in the models folder). However their code was tailored to training on the Recipe1M dataset so we modified it quite a bit, and separated different components for convenience.

# List of files

Very little of the original code from the authors remains intact, but if you look through our repository's commit history you can see how it changed since it was first uploaded.

In models folder:

ingredient_encoder.py: Modified from the ingredient encoder in model_ce.py to remove batch normalization when batch size is 1

sentence_encoder.py: Modifies sentence encoder in model_ce.py. Removed extraneous pooling options and simplified padding/packing into the LSTM

recipe_encoder.py: Mostly unmodified from the recipe encoder in model_ce.py

video_encoder.py: This was not provided to us by the authors so it was written from scratch. It uses a Resnet-34 and LSTM to encode video frames.

sentence_decoder.py: Modified from sentence decoder in model_ce.py to remove extraneous sampling functions and to allow batch size 1.

In datasets folder:

video_datasets.py: Contains the data loader for the Tasty Video Dataset, which was not provided to us, so this was written from scratch

data_loader.py, RECIPE1M.py and Vocabulary.py are legacy scripts from the author's code, in case anyone wishes to replicate their training on Recipe 1M. They are not used in our project.

In data folder:

create_ing_vocab_TVD.py: Script used to generate all the lists and dictionaries used to store vocabulary and ingredient information for the Tasty Video Dataset. Written from scratch.

get_glove_embeds.py: Used to generate glove word embeddings. Written from scratch.

This folder contains important dictionaries generated by create_ing_vocab_TVD.py that are used during training and evaluation to index into the vocabulary and ingredients.

At main level:

train_baseline.py: Script to train the baseline model (sentence encoder, ingredient encoder, recipe RNN, and sentence decoder). Originally based on train_ce.py but heavily modified to allow batch size 1, and additional command line arguments and options, etc.

train_joint_model.py: Script to train the joint model (video and sentence encoder, ingredient encoder, recipe RNN and sentence decoder). Was based on train_ce.py but also heavily modified. Addition of a cosine embedding loss.

evaluate.py: Written from scratch and used to generate predictions along with ground truth for both baseline and joint model.
calculate_metrics.py: Given a txt file containing model output and a txt file containing ground truth, calculates ROUGE, BLEU and Meteor scores.


# Commands

Training original sentence encoder model on Recipe1M:

python train_ce.py --vocab_ing data/new_vocab_ing_3769.pkl --batch_size 2

Training with only sentence encoder (Remember to change file paths in script):

python train_baseline.py --batch_size 1

Training video encoder and sentence encoder jointly (Remember to change file paths in script):

python train_joint_model.py --batch_size 1

Generate predictions (Remember to change model options and paths accordingly):

python evaluate.py

Calculate results (Only have to change paths to predictions and gt):

python calculate_metrics.py

# Requirements

Python 3.7.4
Pytorch 1.6.0
CUDA 8.0
wandb
rouge

Notes:
You can use pip install torch to install pytorch. The version that gets installed will depend on whether you have a GPU+CUDA enabled machine. The same code should work with multiple pytorch versions but I have been using version 1.6.0: https://pytorch.org/docs/1.6.0/ 